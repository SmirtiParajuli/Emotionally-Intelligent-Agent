############################## Emotionally-Intelligent-Agent ##################################
An interactive AI agent that generates emotional speech, classifies audio emotions, and responds with empathy. Built with Python, PyQt5, and machine learning.
    **Author**: Smriti Parajuli  
    **Course**: NLP303 â€“ Speech Processing  
    **Institution**: Media Design School  
    **Submission Date**: June 16, 2025  

---

## ğŸ” Overview

This project implements an interactive, speech-based **Emotionally Sensitive AI Agent** that processes user-input text or recorded voice, classifies the emotional state, and responds empathetically via a dynamic GUI.
The system is built entirely from scratch using classical signal processing and supervised machine learning (Random Forest). No external APIs are used â€” enabling full control and performance evaluation of the core pipeline.

---

## ğŸ§  Project Goals

    - Explore emotional speech processing using handcrafted logic and ML techniques.
    - Build a functional AI agent that adapts its response to user emotions.
    - Evaluate audio-based emotion detection using engineered features.
    - Set the foundation for future integration of large language models (LLMs) and advanced emotional reasoning.

---

## ğŸ“‚ Project Structure
    emotionally-sensitive-agent/
    â”‚
    â”œâ”€â”€ main.py # Main PyQt5 GUI
    â”œâ”€â”€ tts_generator.py # Neutral TTS generation
    â”œâ”€â”€ emotion_modulator.py # Modulate speech (happy, sad, angry, etc.)
    â”œâ”€â”€ emotion_classifier.py # Feature extraction + ML model
    â”œâ”€â”€ agent_response.py # Agent reply logic (hardcoded for now)
    â”‚
    â”œâ”€â”€ /assets # Generated audio files
    â”œâ”€â”€ /report # Trained model, CSVs, and plots
    â”œâ”€â”€ requirements.txt # Python dependencies
    â””â”€â”€ README.md # This file

---

## âš™ï¸ Installation

    ### 1. Install Python  
    Python 3.8â€“3.10 is recommended.
    
    ### 2. Install Dependencies  
    Run:
    ```bash
    pip install -r requirements.txt

## Dependencies:
    PyQt5
    
    librosa
    
    scikit-learn
    
    matplotlib
    
    numpy
    
    soundfile
    
    sounddevice
    
    noisereduce
    
    speechrecognition

## Launch the Application:
    python main.py

---

ğŸ’¡ Key Features

    âœ… Generate neutral speech from user-entered sentences
    
    âœ… Apply pitch, speed, and gain changes for emotional modulation
    
    âœ… Extract MFCC, ZCR, spectral centroid, RMS, duration, and dominant frequency
    
    âœ… Train and use RandomForest for emotion classification
    
    âœ… Accept live voice input or uploaded .wav files
    
    âœ… Generate empathetic replies using emoji-enhanced response logic
    
    âœ… Visualize waveform, MFCCs, and spectrogram in PyQt5 GUI
    
    âœ… Includes emotion playback, file browsing, reset/clear, and confidence scores

---

ğŸ¤– Agent Logic (Current Status & Future Plan)

    The agentâ€™s response system is currently hardcoded with simple rules based on the predicted emotion â€” this is intentional for performance and interpretability testing.
    
    ğŸ”­ Future Upgrades (Planned for Phase 2):
    ğŸ”Œ LLM / ChatGPT API Integration: Enable nuanced, adaptive conversations
    
    ğŸ§  Emotion-Conditioned Prompting: Vary prompts based on detected emotion
    
    ğŸ­ Agent Persona Profiles: Custom personalities (friendly, professional, humorous)
    
    ğŸŒ¡ï¸ Emotion Intensity Scaling: Modify tone based on classifier confidence
    
    These enhancements will transform the agent from a rule-based bot to a truly empathetic digital assistant.
---

ğŸ–¥ï¸ How to Use the GUI

    Enter a sentence and click Generate & Train.
    
    Choose an emotion from the dropdown to hear its modulated version.
    
    Upload a .wav file or use Record Voice to input speech.
    
    Click Predict Emotion.
    
    View the predicted emotion, dominant frequency, waveform, MFCCs, spectrogram, and agent reply.
---

ğŸ“Œ Example Output
    
    Input: Neutral speech modulated as "Happy"
    â†’ Predicted Emotion: Happy
    â†’ Dominant Frequency: 265.62 Hz
    â†’ Agent Response: I have something to tell you. ğŸ˜Š That really makes me smile!
    
    Top Predictions:
    - Happy: 58%
    - Nervous: 18%
    - Angry: 10%
    
---
ğŸš€ Future Enhancements

    ğŸŒ Multilingual support
    
    ğŸ§  Deep learning models (CNN, RNN) for emotion classification
    
    ğŸ›ï¸ Real-time processing optimization
    
    ğŸ‘¤ Avatar-based agent display
    
    ğŸ”Š Emotion blending and layered tone shifts

---
 Acknowledgements
 
    Librosa â€“ Audio processing
    
    Scikit-learn â€“ ML algorithms
    
    PyQt5 â€“ GUI framework
    
    SpeechRecognition â€“ Voice input
    
    Noisereduce â€“ Denoising   
    
ğŸ‘©â€ğŸ’» Author
Smriti Parajuli
Bachelor of Software Engineering (AI)
Media Design School â€“ 2025
GitHub

---

If you'd like this exported as a downloadable `.md` file or want a shorter version for LinkedIn or your website, just say the word!
